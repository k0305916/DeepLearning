# Sigmoid Func Compares with Step Func
## Difference
1. 平滑性。
    * sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化。
    * Step函数则是以0为界，输出发生急剧性的变化。
    * sigmoid函数的平滑性对神经网络的学习具有重要意义。
2. sigmoid函数可以返回实数，而step函数则只能返回0 or 1.
    * 即，感知机中神经元之间流动的是0 or 1的二元信号
    * 而，神经网络中流动的是连续的实数值信号。
## Same
1. 输出为[0,1]。
2. 都为非线性函数。

# Note
* 神经网络的激活函数必须使用非线性函数。因为，使用线性函数的话，加深神经网络的层数就没有意义了。
* 为了发挥叠加层所带来的优势，激活函数则必须使用非线性函数。

# Design Output
* 机器学习的问题大致可以分为分类问题和回归问题。
* 恒等函数：会将输入按原样输出，对于输入的信息，不加以任何改动地直接输出。
* softmax函数，用于分类问题中。softmax函数的分子是输入信号a k 的指数函数，分母是所有输入信号的指数 
  1. 函数的和。输出总和为1，是softmax函数的一个重要性质。
  2. 即便使用了softmax函数，各个元素之间的大小关系也不会改变。因为y=exp(x)是单调递增函数，并不会改变元素的大小关系。
    $$
    Y_k = exp(a_k) / \sum_{i=1}^{n}exp(a_i)
    $$
* 一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。因此，神经网络在进行分类时，输出层的softmax函数可以省略。